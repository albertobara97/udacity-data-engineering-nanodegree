{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "df43137509eb68088699105a61565dad871be4f70351b14b46e26560cb30e335"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Capstone project - Analytics in agriculture\n",
    "\n",
    "### In this file, we can find the ETL process that our project follows to go from the raw data located in 'data/' to the curated data stored in our rdbms. For this first version the rdbms will be PostgreSQL"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import time\n",
    "import configparser\n",
    "import json"
   ]
  },
  {
   "source": [
    "# 1. Extraction\n",
    "\n",
    "### We are not starting from the very first stage. The extraction phase begins when downloading the data from the database, but since this first step needs to be done yearly due to de refresh schedule that this data is following, we did a manual step before the one described below (Manual step: download files > uncompress files)\n",
    "\n",
    "### After the short explanation, we proceed with the extraction of the data. The data that our source provides are csv files. Since, the data is completely untouched, we will need to select the files/tables that are useful for our project and rearrange the structure of the columns because as we will see during the etl, the structure given is optimized for storage but not for a more advanced data model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Alberto\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py:3155: DtypeWarning: Columns (8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56) have mixed types.Specify dtype option on import or set low_memory=False.\n  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "crops_data = pd.read_csv(\"data/Production_Crops_E_All_Data.csv\", encoding=\"ANSI\")\n",
    "trade_data = pd.read_csv(\"data/Trade_Crops_Livestock_E_All_Data.csv\", encoding=\"ANSI\")\n",
    "crops_flags = pd.read_csv(\"data/Production_Crops_E_Flags.csv\", encoding=\"ANSI\")\n",
    "trade_flags = pd.read_csv(\"data/Trade_Crops_Livestock_E_Flags.csv\", encoding=\"ANSI\")\n",
    "\n",
    "with open(\"credentials/redshift.json\", 'r') as j:\n",
    "    redshift = json.loads(j.read())"
   ]
  },
  {
   "source": [
    "# 2. Transformation\n",
    "\n",
    "## Creation of dimension tables"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_countries = crops_data[[\"Area Code\", \"Area\"]].append(trade_data[[\"Area Code\", \"Area\"]]).drop_duplicates()\n",
    "dim_items = crops_data[[\"Item Code\", \"Item\"]].drop_duplicates()\n",
    "dim_elements = crops_data[[\"Element Code\", \"Element\"]].append(trade_data[[\"Element Code\", \"Element\"]]).drop_duplicates()\n",
    "dim_flags = crops_flags.append(trade_flags).drop_duplicates()"
   ]
  },
  {
   "source": [
    "## Clean dataframes\n",
    "\n",
    "### Trade data has mixed crops and products data. to increase the performance of the next steps, first we will need to remove the rows that are not crops.\n",
    "\n",
    "### Dimensions contain lots of duplciated data, therefore they will be trimmed as well"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_data = trade_data[trade_data[\"Item Code\"].isin(dim_items[\"Item Code\"])]\n",
    "#temp_items = trade_data[\"Item\"].drop_duplicates()\n",
    "#temp_items"
   ]
  },
  {
   "source": [
    "## rearrange the dataframe structures and creation of the fact table\n",
    "\n",
    "### The design of this structure, will make the data grow horizontally, but for our SQL schema we can't keep a schema that is growing into this direction, so to rearrange the tables we have divided the data into 2 groups: keys and values. \n",
    "* keys: data that will be repeated after each iteration and serves as an identifier for the values\n",
    "* values: data reported yearly and makes the dataframe grow each year 2 columns more"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "evaluated from crops_data:  Y1961\n",
      "evaluated from crops_data:  Y1962\n",
      "evaluated from crops_data:  Y1963\n",
      "evaluated from crops_data:  Y1964\n",
      "evaluated from crops_data:  Y1965\n",
      "evaluated from crops_data:  Y1966\n",
      "evaluated from crops_data:  Y1967\n",
      "evaluated from crops_data:  Y1968\n",
      "evaluated from crops_data:  Y1969\n",
      "evaluated from crops_data:  Y1970\n",
      "evaluated from crops_data:  Y1971\n",
      "evaluated from crops_data:  Y1972\n",
      "evaluated from crops_data:  Y1973\n",
      "evaluated from crops_data:  Y1974\n",
      "evaluated from crops_data:  Y1975\n",
      "evaluated from crops_data:  Y1976\n",
      "evaluated from crops_data:  Y1977\n",
      "evaluated from crops_data:  Y1978\n",
      "evaluated from crops_data:  Y1979\n",
      "evaluated from crops_data:  Y1980\n",
      "evaluated from crops_data:  Y1981\n",
      "evaluated from crops_data:  Y1982\n",
      "evaluated from crops_data:  Y1983\n",
      "evaluated from crops_data:  Y1984\n",
      "evaluated from crops_data:  Y1985\n",
      "evaluated from crops_data:  Y1986\n",
      "evaluated from crops_data:  Y1987\n",
      "evaluated from crops_data:  Y1988\n",
      "evaluated from crops_data:  Y1989\n",
      "evaluated from crops_data:  Y1990\n",
      "evaluated from crops_data:  Y1991\n",
      "evaluated from crops_data:  Y1992\n",
      "evaluated from crops_data:  Y1993\n",
      "evaluated from crops_data:  Y1994\n",
      "evaluated from crops_data:  Y1995\n",
      "evaluated from crops_data:  Y1996\n",
      "evaluated from crops_data:  Y1997\n",
      "evaluated from crops_data:  Y1998\n",
      "evaluated from crops_data:  Y1999\n",
      "evaluated from crops_data:  Y2000\n",
      "evaluated from crops_data:  Y2001\n",
      "evaluated from crops_data:  Y2002\n",
      "evaluated from crops_data:  Y2003\n",
      "evaluated from crops_data:  Y2004\n",
      "evaluated from crops_data:  Y2005\n",
      "evaluated from crops_data:  Y2006\n",
      "evaluated from crops_data:  Y2007\n",
      "evaluated from crops_data:  Y2008\n",
      "evaluated from crops_data:  Y2009\n",
      "evaluated from crops_data:  Y2010\n",
      "evaluated from crops_data:  Y2011\n",
      "evaluated from crops_data:  Y2012\n",
      "evaluated from crops_data:  Y2013\n",
      "evaluated from crops_data:  Y2014\n",
      "evaluated from crops_data:  Y2015\n",
      "evaluated from crops_data:  Y2016\n",
      "evaluated from crops_data:  Y2017\n",
      "evaluated from crops_data:  Y2018\n",
      "evaluated from crops_data:  Y2019\n",
      "evaluated from trade_data:  Y1961\n",
      "evaluated from trade_data:  Y1962\n",
      "evaluated from trade_data:  Y1963\n",
      "evaluated from trade_data:  Y1964\n",
      "evaluated from trade_data:  Y1965\n",
      "evaluated from trade_data:  Y1966\n",
      "evaluated from trade_data:  Y1967\n",
      "evaluated from trade_data:  Y1968\n",
      "evaluated from trade_data:  Y1969\n",
      "evaluated from trade_data:  Y1970\n",
      "evaluated from trade_data:  Y1971\n",
      "evaluated from trade_data:  Y1972\n",
      "evaluated from trade_data:  Y1973\n",
      "evaluated from trade_data:  Y1974\n",
      "evaluated from trade_data:  Y1975\n",
      "evaluated from trade_data:  Y1976\n",
      "evaluated from trade_data:  Y1977\n",
      "evaluated from trade_data:  Y1978\n",
      "evaluated from trade_data:  Y1979\n",
      "evaluated from trade_data:  Y1980\n",
      "evaluated from trade_data:  Y1981\n",
      "evaluated from trade_data:  Y1982\n",
      "evaluated from trade_data:  Y1983\n",
      "evaluated from trade_data:  Y1984\n",
      "evaluated from trade_data:  Y1985\n",
      "evaluated from trade_data:  Y1986\n",
      "evaluated from trade_data:  Y1987\n",
      "evaluated from trade_data:  Y1988\n",
      "evaluated from trade_data:  Y1989\n",
      "evaluated from trade_data:  Y1990\n",
      "evaluated from trade_data:  Y1991\n",
      "evaluated from trade_data:  Y1992\n",
      "evaluated from trade_data:  Y1993\n",
      "evaluated from trade_data:  Y1994\n",
      "evaluated from trade_data:  Y1995\n",
      "evaluated from trade_data:  Y1996\n",
      "evaluated from trade_data:  Y1997\n",
      "evaluated from trade_data:  Y1998\n",
      "evaluated from trade_data:  Y1999\n",
      "evaluated from trade_data:  Y2000\n",
      "evaluated from trade_data:  Y2001\n",
      "evaluated from trade_data:  Y2002\n",
      "evaluated from trade_data:  Y2003\n",
      "evaluated from trade_data:  Y2004\n",
      "evaluated from trade_data:  Y2005\n",
      "evaluated from trade_data:  Y2006\n",
      "evaluated from trade_data:  Y2007\n",
      "evaluated from trade_data:  Y2008\n",
      "evaluated from trade_data:  Y2009\n",
      "evaluated from trade_data:  Y2010\n",
      "evaluated from trade_data:  Y2011\n",
      "evaluated from trade_data:  Y2012\n",
      "evaluated from trade_data:  Y2013\n",
      "evaluated from trade_data:  Y2014\n",
      "evaluated from trade_data:  Y2015\n",
      "evaluated from trade_data:  Y2016\n",
      "evaluated from trade_data:  Y2017\n",
      "evaluated from trade_data:  Y2018\n",
      "evaluated from trade_data:  Y2019\n",
      "elapsed time:  121.13856148719788\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "raw_crop_keys = crops_data[[\"Area Code\", \"Area\", \"Item Code\", \"Item\", \"Element Code\", \"Element\", \"Unit\"]]\n",
    "raw_crop_values = crops_data.drop(labels = [\"Area Code\", \"Area\", \"Item Code\", \"Item\", \"Element Code\", \"Element\", \"Unit\"], axis = 1)\n",
    "\n",
    "raw_trade_keys = trade_data[[\"Area Code\", \"Area\", \"Item Code\", \"Item\", \"Element Code\", \"Element\", \"Unit\"]]\n",
    "raw_trade_values = trade_data.drop(labels = [\"Area Code\", \"Area\", \"Item Code\", \"Item\", \"Element Code\", \"Element\", \"Unit\"], axis = 1)\n",
    "\n",
    "if(len(raw_crop_values.columns) % 2 == 1):\n",
    "    print(raw_crop_values.columns)\n",
    "    raise Exception(\"Unexpected column found, columns number must be even as they consist of pairs. Please check out the dataframe structure\")\n",
    "\n",
    "if(len(raw_trade_values.columns) % 2 == 1):\n",
    "    print(raw_trade_values.columns)\n",
    "    raise Exception(\"Unexpected column found, columns number must be even as they consist of pairs. Please check out the dataframe structure\")\n",
    "\n",
    "fact_crops_data = pd.DataFrame(columns = [\"Area Code\", \"Area\", \"Item Code\", \"Item\", \"Element Code\", \"Element\", \"Unit\", \"Year\", \"Value\", \"Flag\"])\n",
    "\n",
    "fact_trade_data = pd.DataFrame(columns = [\"Area Code\", \"Area\", \"Item Code\", \"Item\", \"Element Code\", \"Element\", \"Unit\", \"Year\", \"Value\", \"Flag\"])\n",
    "\n",
    "for A, B in zip(*[iter(raw_crop_values)]*2):\n",
    "    temp_aux_crops = raw_crop_keys.append(raw_crop_values[[A, B]]).rename(columns = {A: \"Value\", B: \"Flag\"})\n",
    "    print(\"evaluated from crops_data: \", A)\n",
    "    fact_crops_data = fact_crops_data.append(temp_aux_crops)\n",
    "\n",
    "for A, B in zip(*[iter(raw_trade_values)]*2):\n",
    "    temp_aux_trade = raw_trade_keys.append(raw_trade_values[[A, B]]).rename(columns = {A: \"Value\", B: \"Flag\"})\n",
    "    print(\"evaluated from trade_data: \", A)\n",
    "    fact_trade_data = fact_trade_data.append(temp_aux_trade)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"elapsed time: \", end - start)"
   ]
  },
  {
   "source": [
    "# 3. Load\n",
    "\n",
    "## Load the tables into our redshift cluster"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'endpoint': 'redshift-cluster.crisbfgqf52j.eu-west-2.redshift.amazonaws.com', 'database': 'dev', 'username': 'awsuser', 'password': 'Udacity59', 'port': 5439}\n"
     ]
    }
   ],
   "source": [
    "conn = psycopg2.connect(f\"host={redshift['endpoint']} dbname={redshift['database']} user={redshift['username']} password={redshift['password']} port={redshift['port']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\"5439\"'"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redshift_credentials['port']"
   ]
  }
 ]
}