{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "df43137509eb68088699105a61565dad871be4f70351b14b46e26560cb30e335"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Capstone project - Analytics in agriculture\n",
    "\n",
    "### In this file, we can find the ETL process that our project follows to go from the raw data located in 'data/' to the curated data stored in our rdbms. For this first version the rdbms will be PostgreSQL"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import extras\n",
    "import pandas as pd\n",
    "import time\n",
    "import configparser\n",
    "import json\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import numpy as np"
   ]
  },
  {
   "source": [
    "# 1. Extraction\n",
    "\n",
    "### We are not starting from the very first stage. The extraction phase begins when downloading the data from the database, but since this first step needs to be done yearly due to de refresh schedule that this data is following, we did a manual step before the one described below (Manual step: download files > uncompress files)\n",
    "\n",
    "### After the short explanation, we proceed with the extraction of the data. The data that our source provides are csv files. Since, the data is completely untouched, we will need to select the files/tables that are useful for our project and rearrange the structure of the columns because as we will see during the etl, the structure given is optimized for storage but not for a more advanced data model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Alberto\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py:3155: DtypeWarning: Columns (8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56) have mixed types.Specify dtype option on import or set low_memory=False.\n  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "# Load original tabular data\n",
    "crops_data = pd.read_csv(\"data/Production_Crops_E_All_Data.csv\", encoding=\"ANSI\")\n",
    "trade_data = pd.read_csv(\"data/Trade_Crops_Livestock_E_All_Data.csv\", encoding=\"ANSI\")\n",
    "\n",
    "# Load flags used in main tables (crops_data & trade_data)\n",
    "crops_flags = pd.read_csv(\"data/Production_Crops_E_Flags.csv\", encoding=\"ANSI\")\n",
    "trade_flags = pd.read_csv(\"data/Trade_Crops_Livestock_E_Flags.csv\", encoding=\"ANSI\")\n",
    "\n",
    "# Load redshift credentials\n",
    "with open(\"credentials/redshift.json\", 'r') as j:\n",
    "    redshift = json.loads(j.read())\n",
    "\n",
    "# Load aws credentials\n",
    "with open(\"credentials/aws.json\", 'r') as j:\n",
    "    aws = json.loads(j.read())\n",
    "\n",
    "# Load s3 credentials \n",
    "with open(\"credentials/s3.json\", 'r') as j:\n",
    "    s3 = json.loads(j.read())\n",
    "\n",
    "# Create object for further use when connecting to s3 bucket\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Delete temporary variable j\n",
    "del j"
   ]
  },
  {
   "source": [
    "# 2. Transformation\n",
    "\n",
    "## Creation of dimension tables"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dimension table countries\n",
    "dim_countries = crops_data[[\"Area Code\", \"Area\"]].append(trade_data[[\"Area Code\", \"Area\"]]).drop_duplicates()\n",
    "\n",
    "#Create dimension table items\n",
    "dim_items = crops_data[[\"Item Code\", \"Item\"]].drop_duplicates()\n",
    "\n",
    "# Create dimension table elements\n",
    "dim_elements = crops_data[[\"Element Code\", \"Element\"]].append(trade_data[[\"Element Code\", \"Element\"]]).drop_duplicates()\n",
    "\n",
    "#Create dimension table flags\n",
    "dim_flags = crops_flags.append(trade_flags).drop_duplicates()\n",
    "\n",
    "#Delete original flags dataframes\n",
    "del crops_flags, trade_flags"
   ]
  },
  {
   "source": [
    "## Clean dataframes\n",
    "\n",
    "### Trade data has mixed crops and products data. to increase the performance of the next steps, first we will need to remove the rows that are not crops.\n",
    "\n",
    "### Dimensions contain lots of duplciated data, therefore they will be trimmed as well"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_data = trade_data[trade_data[\"Item Code\"].isin(dim_items[\"Item Code\"])]"
   ]
  },
  {
   "source": [
    "## Modify Flags dataframe\n",
    "\n",
    "### Flags table has \"blank\" primary key string associated to \"Official data\", but in the  fact table the value is blank. So it is needed to change the string \"blank\" to a blank string"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_flags = dim_flags.replace(\"<blank>\", \"\")"
   ]
  },
  {
   "source": [
    "## rearrange the dataframe structures and creation of the fact table\n",
    "\n",
    "### The design of this structure, will make the data grow horizontally, but for our SQL schema we can't keep a schema that is growing into this direction, so to rearrange the tables we have divided the data into 2 groups: keys and values. \n",
    "* keys: data that will be repeated after each iteration and serves as an identifier for the values\n",
    "* values: data reported yearly and makes the dataframe grow each year 2 columns more"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-37-9b6326088d8e>:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_aux_crops['Year'] = A[1:]\n",
      "<ipython-input-37-9b6326088d8e>:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_aux_crops['Value'] = raw_crop_values[[A]]\n",
      "<ipython-input-37-9b6326088d8e>:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_aux_crops['Flag'] = raw_crop_values[[B]]\n",
      "evaluated from crops_data:  Y1961\n",
      "evaluated from crops_data:  Y1962\n",
      "evaluated from crops_data:  Y1963\n",
      "evaluated from crops_data:  Y1964\n",
      "evaluated from crops_data:  Y1965\n",
      "evaluated from crops_data:  Y1966\n",
      "evaluated from crops_data:  Y1967\n",
      "evaluated from crops_data:  Y1968\n",
      "evaluated from crops_data:  Y1969\n",
      "evaluated from crops_data:  Y1970\n",
      "evaluated from crops_data:  Y1971\n",
      "evaluated from crops_data:  Y1972\n",
      "evaluated from crops_data:  Y1973\n",
      "evaluated from crops_data:  Y1974\n",
      "evaluated from crops_data:  Y1975\n",
      "evaluated from crops_data:  Y1976\n",
      "evaluated from crops_data:  Y1977\n",
      "evaluated from crops_data:  Y1978\n",
      "evaluated from crops_data:  Y1979\n",
      "evaluated from crops_data:  Y1980\n",
      "evaluated from crops_data:  Y1981\n",
      "evaluated from crops_data:  Y1982\n",
      "evaluated from crops_data:  Y1983\n",
      "evaluated from crops_data:  Y1984\n",
      "evaluated from crops_data:  Y1985\n",
      "evaluated from crops_data:  Y1986\n",
      "evaluated from crops_data:  Y1987\n",
      "evaluated from crops_data:  Y1988\n",
      "evaluated from crops_data:  Y1989\n",
      "evaluated from crops_data:  Y1990\n",
      "evaluated from crops_data:  Y1991\n",
      "evaluated from crops_data:  Y1992\n",
      "evaluated from crops_data:  Y1993\n",
      "evaluated from crops_data:  Y1994\n",
      "evaluated from crops_data:  Y1995\n",
      "evaluated from crops_data:  Y1996\n",
      "evaluated from crops_data:  Y1997\n",
      "evaluated from crops_data:  Y1998\n",
      "evaluated from crops_data:  Y1999\n",
      "evaluated from crops_data:  Y2000\n",
      "evaluated from crops_data:  Y2001\n",
      "evaluated from crops_data:  Y2002\n",
      "evaluated from crops_data:  Y2003\n",
      "evaluated from crops_data:  Y2004\n",
      "evaluated from crops_data:  Y2005\n",
      "evaluated from crops_data:  Y2006\n",
      "evaluated from crops_data:  Y2007\n",
      "evaluated from crops_data:  Y2008\n",
      "evaluated from crops_data:  Y2009\n",
      "evaluated from crops_data:  Y2010\n",
      "evaluated from crops_data:  Y2011\n",
      "evaluated from crops_data:  Y2012\n",
      "evaluated from crops_data:  Y2013\n",
      "evaluated from crops_data:  Y2014\n",
      "evaluated from crops_data:  Y2015\n",
      "evaluated from crops_data:  Y2016\n",
      "evaluated from crops_data:  Y2017\n",
      "evaluated from crops_data:  Y2018\n",
      "evaluated from crops_data:  Y2019\n",
      "<ipython-input-37-9b6326088d8e>:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_aux_trade['Year'] = A[1:]\n",
      "<ipython-input-37-9b6326088d8e>:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_aux_trade['Value'] = raw_trade_values[[A]]\n",
      "<ipython-input-37-9b6326088d8e>:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_aux_trade['Flag'] = raw_trade_values[[B]]\n",
      "evaluated from trade_data:  Y1961\n",
      "evaluated from trade_data:  Y1962\n",
      "evaluated from trade_data:  Y1963\n",
      "evaluated from trade_data:  Y1964\n",
      "evaluated from trade_data:  Y1965\n",
      "evaluated from trade_data:  Y1966\n",
      "evaluated from trade_data:  Y1967\n",
      "evaluated from trade_data:  Y1968\n",
      "evaluated from trade_data:  Y1969\n",
      "evaluated from trade_data:  Y1970\n",
      "evaluated from trade_data:  Y1971\n",
      "evaluated from trade_data:  Y1972\n",
      "evaluated from trade_data:  Y1973\n",
      "evaluated from trade_data:  Y1974\n",
      "evaluated from trade_data:  Y1975\n",
      "evaluated from trade_data:  Y1976\n",
      "evaluated from trade_data:  Y1977\n",
      "evaluated from trade_data:  Y1978\n",
      "evaluated from trade_data:  Y1979\n",
      "evaluated from trade_data:  Y1980\n",
      "evaluated from trade_data:  Y1981\n",
      "evaluated from trade_data:  Y1982\n",
      "evaluated from trade_data:  Y1983\n",
      "evaluated from trade_data:  Y1984\n",
      "evaluated from trade_data:  Y1985\n",
      "evaluated from trade_data:  Y1986\n",
      "evaluated from trade_data:  Y1987\n",
      "evaluated from trade_data:  Y1988\n",
      "evaluated from trade_data:  Y1989\n",
      "evaluated from trade_data:  Y1990\n",
      "evaluated from trade_data:  Y1991\n",
      "evaluated from trade_data:  Y1992\n",
      "evaluated from trade_data:  Y1993\n",
      "evaluated from trade_data:  Y1994\n",
      "evaluated from trade_data:  Y1995\n",
      "evaluated from trade_data:  Y1996\n",
      "evaluated from trade_data:  Y1997\n",
      "evaluated from trade_data:  Y1998\n",
      "evaluated from trade_data:  Y1999\n",
      "evaluated from trade_data:  Y2000\n",
      "evaluated from trade_data:  Y2001\n",
      "evaluated from trade_data:  Y2002\n",
      "evaluated from trade_data:  Y2003\n",
      "evaluated from trade_data:  Y2004\n",
      "evaluated from trade_data:  Y2005\n",
      "evaluated from trade_data:  Y2006\n",
      "evaluated from trade_data:  Y2007\n",
      "evaluated from trade_data:  Y2008\n",
      "evaluated from trade_data:  Y2009\n",
      "evaluated from trade_data:  Y2010\n",
      "evaluated from trade_data:  Y2011\n",
      "evaluated from trade_data:  Y2012\n",
      "evaluated from trade_data:  Y2013\n",
      "evaluated from trade_data:  Y2014\n",
      "evaluated from trade_data:  Y2015\n",
      "evaluated from trade_data:  Y2016\n",
      "evaluated from trade_data:  Y2017\n",
      "evaluated from trade_data:  Y2018\n",
      "evaluated from trade_data:  Y2019\n",
      "elapsed time:  58.13883996009827\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "#Separete keys from values, keys will be repeated when the data will be appended, values will be iterated to chain the data vertically (rows) instead of horizontally (columns)\n",
    "raw_crop_keys = crops_data[[\"Area Code\", \"Item Code\", \"Element Code\", \"Unit\"]]\n",
    "raw_crop_values = crops_data.drop(labels = [\"Area Code\", \"Area\", \"Item Code\", \"Item\", \"Element Code\", \"Element\", \"Unit\"], axis = 1)\n",
    "\n",
    "#Separete keys from values, keys will be repeated when the data will be appended, values will be iterated to chain the data vertically (rows) instead of horizontally (columns)\n",
    "raw_trade_keys = trade_data[[\"Area Code\", \"Item Code\", \"Element Code\", \"Unit\"]]\n",
    "raw_trade_values = trade_data.drop(labels = [\"Area Code\", \"Area\", \"Item Code\", \"Item\", \"Element Code\", \"Element\", \"Unit\"], axis = 1)\n",
    "\n",
    "# Simple data quality check. Values are always composed by (value, flag) pairs, if number of columns is odd, there is something wrong\n",
    "if(len(raw_crop_values.columns) % 2 == 1):\n",
    "    print(raw_crop_values.columns)\n",
    "    raise Exception(\"Unexpected column found, columns number must be even as they consist of pairs. Please check out the dataframe structure\")\n",
    "\n",
    "# Simple data quality check. Values are always composed by (value, flag) pairs, if number of columns is odd, there is something wrong\n",
    "if(len(raw_trade_values.columns) % 2 == 1):\n",
    "    print(raw_trade_values.columns)\n",
    "    raise Exception(\"Unexpected column found, columns number must be even as they consist of pairs. Please check out the dataframe structure\")\n",
    "\n",
    "# create empty dataframe for fact table\n",
    "fact_crops = pd.DataFrame(columns = [\"Area Code\", \"Item Code\", \"Element Code\", \"Unit\", \"Year\", \"Value\", \"Flag\"])\n",
    "\n",
    "# create empty dataframe for fact table\n",
    "fact_trade = pd.DataFrame(columns = [\"Area Code\", \"Item Code\", \"Element Code\", \"Unit\", \"Year\", \"Value\", \"Flag\"])\n",
    "\n",
    "# Iterate over each year of values to append the data to the fact table\n",
    "for A, B in zip(*[iter(raw_crop_values)]*2):\n",
    "    temp_aux_crops = raw_crop_keys\n",
    "    temp_aux_crops['Year'] = A[1:]\n",
    "    temp_aux_crops['Value'] = raw_crop_values[[A]]\n",
    "    temp_aux_crops['Flag'] = raw_crop_values[[B]]\n",
    "    print(\"evaluated from crops_data: \", A)\n",
    "    fact_crops = fact_crops.append(temp_aux_crops)\n",
    "\n",
    "# Iterate over each year of values to append the data to the fact table\n",
    "for A, B in zip(*[iter(raw_trade_values)]*2):\n",
    "    temp_aux_trade = raw_trade_keys\n",
    "    temp_aux_trade['Year'] = A[1:]\n",
    "    temp_aux_trade['Value'] = raw_trade_values[[A]]\n",
    "    temp_aux_trade['Flag'] = raw_trade_values[[B]]\n",
    "    print(\"evaluated from trade_data: \", A)\n",
    "    fact_trade = fact_trade.append(temp_aux_trade)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "# Delete temporary variables\n",
    "del A, B, raw_crop_keys, raw_crop_values, raw_trade_keys, raw_trade_values, temp_aux_crops, temp_aux_trade\n",
    "\n",
    "# Delete original dataframes\n",
    "del crops_data, trade_data\n",
    "\n",
    "print(\"elapsed time: \", end - start)\n",
    "\n",
    "# Delete chrono temporary variables\n",
    "del start, end"
   ]
  },
  {
   "source": [
    "## Combine the 2 main (fact) tables into one, appending each other.\n",
    "\n",
    "### As backup and for logging purposes, we will also store the 2 fact tables."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append both fact tables into a common one\n",
    "fact_fao = fact_crops.append(fact_trade)\n",
    "\n",
    "# Store crops fact table as backup\n",
    "fact_crops.to_csv(\"D:/Documents/GitHub/udacity-data-engineering-nanodegree/Capstone project - Analytics in agriculture/data/fact_crops.csv\", index=False, header=False)\n",
    "\n",
    "# Store trade fact table as backup\n",
    "fact_trade.to_csv(\"D:/Documents/GitHub/udacity-data-engineering-nanodegree/Capstone project - Analytics in agriculture/data/fact_trade.csv\", index=False, header=False)\n",
    "\n",
    "# Store main fact table for further use\n",
    "fact_fao.to_csv(\"D:/Documents/GitHub/udacity-data-engineering-nanodegree/Capstone project - Analytics in agriculture/data/fact_fao.csv\", index=False, header=False)"
   ]
  },
  {
   "source": [
    "# 3. Load\n",
    "\n",
    "## Load the tables into our redshift cluster"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the redshift cluster\n",
    "conn = psycopg2.connect(f\"host={redshift['endpoint']} dbname={redshift['database']} user={redshift['username']} password={redshift['password']} port={redshift['port']}\")\n",
    "conn.autocommit = True\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "source": [
    "## Drop tables if needed"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_table(table):\n",
    "    '''\n",
    "    Drops the table on the Redshift database\n",
    "\n",
    "    Parameters:\n",
    "        - table: Name of the table to delete\n",
    "    '''\n",
    "\n",
    "    try: \n",
    "        cur.execute(f\"DROP TABLE {table};\")\n",
    "    except psycopg2.errors.ProgrammingError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Table \"countries\" does not exist\n",
      "\n",
      "Table \"elements\" does not exist\n",
      "\n",
      "Table \"flags\" does not exist\n",
      "\n",
      "Table \"items\" does not exist\n",
      "\n",
      "Table \"fact_fao\" does not exist\n",
      "\n",
      "Table \"fact_crops\" does not exist\n",
      "\n",
      "Table \"fact_trade\" does not exist\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Only execute these functions if the data is wrong in the database and you need to run again the ETL or you want to repopulate the tables\n",
    "\n",
    "drop_table(\"countries\")\n",
    "drop_table(\"elements\")\n",
    "drop_table(\"elements\")\n",
    "drop_table(\"items\")\n",
    "drop_table(\"fact_fao\")\n"
   ]
  },
  {
   "source": [
    "## Create tables if needed"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create if not exsits fact table\n",
    "cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS fact_fao(\n",
    "        Area_code int,\n",
    "        Item_Code int,\n",
    "        Element_Code int,\n",
    "        Unit varchar(50),\n",
    "        Year int,\n",
    "        Value float,\n",
    "        Flag varchar(25)\n",
    "    );\n",
    "\"\"\")\n",
    "\n",
    "# Create Countries dimension table if not exists\n",
    "cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS countries(\n",
    "        Area_Code int not null UNIQUE PRIMARY KEY,\n",
    "        Area varchar(100)\n",
    "    );\n",
    "\"\"\")\n",
    "\n",
    "# Create Elements dimension table if not exists\n",
    "cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS elements(\n",
    "        Element_Code int not null UNIQUE PRIMARY KEY,\n",
    "        Element varchar(100)\n",
    "    );\n",
    "\"\"\")\n",
    "\n",
    "# Create Flags dimension table if not exists\n",
    "cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS flags(\n",
    "        Flag varchar(25) UNIQUE PRIMARY KEY,\n",
    "        Description varchar(100)\n",
    "    );\n",
    "\"\"\")\n",
    "\n",
    "# Create Items dimension table if not exists\n",
    "cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS items(\n",
    "        Item_Code int not null UNIQUE PRIMARY KEY,\n",
    "        Item varchar(100)\n",
    "    );\n",
    "\"\"\")"
   ]
  },
  {
   "source": [
    "## Load data into dimension tables"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dimension tables lodaded!\nelapsed time:  0.7024111747741699\n"
     ]
    }
   ],
   "source": [
    "# Load dimension tables\n",
    "start = time.time()\n",
    "\n",
    "try:\n",
    "    psycopg2.extras.execute_values(cur, \"INSERT INTO countries VALUES %s;\", dim_countries.itertuples(index=False))\n",
    "    \n",
    "    psycopg2.extras.execute_values(cur, \"INSERT INTO flags VALUES %s\", dim_flags.itertuples(index=False))\n",
    "\n",
    "    psycopg2.extras.execute_values(cur, \"INSERT INTO elements VALUES %s\", dim_elements.itertuples(index=False))\n",
    "\n",
    "    psycopg2.extras.execute_values(cur, \"INSERT INTO items VALUES %s\", dim_items.itertuples(index=False))\n",
    "\n",
    "except: \n",
    "    conn.rollback()\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Dimension tables lodaded!\")\n",
    "print(\"elapsed time: \", end - start)\n",
    "\n",
    "# Delete chrono temporary variables\n",
    "del start, end"
   ]
  },
  {
   "source": [
    "## Upload fact table to S3 bucket\n",
    "\n",
    "### `COPY` command from Redshift is only possible if fetching data from S3 bucket. So we proceed copying the file into S3"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "elapsed time:  33.124932289123535\n"
     ]
    }
   ],
   "source": [
    "# Load fact_fao table\n",
    "start = time.time()\n",
    "\n",
    "try:\n",
    "    response = s3_client.upload_file(\"D:/Documents/GitHub/udacity-data-engineering-nanodegree/Capstone project - Analytics in agriculture/data/fact_fao.csv\", \"arn:aws:s3:eu-west-2:412813684759:accesspoint/awsaccess\", \"fact_fao.csv\")\n",
    "except ClientError as e:\n",
    "    logging.error(e)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"elapsed time: \", end - start)"
   ]
  },
  {
   "source": [
    "## Load data into fact table"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "elapsed time:  13.558779954910278\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "try:\n",
    "    cur.execute(f\"\"\"\n",
    "        COPY fact_fao\n",
    "        FROM '{s3[\"base-connection\"]}{s3[\"fact_fao\"]}'\n",
    "        ACCESS_KEY_ID '{aws[\"ACCESS_KEY_ID\"]}'\n",
    "        SECRET_ACCESS_KEY '{aws[\"SECRET_ACCESS_KEY\"]}'\n",
    "        DELIMITER ','\n",
    "    \"\"\")\n",
    "except ClientError as e:\n",
    "    logging.error(e)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"elapsed time: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "nº of rows: 9732699\n"
     ]
    }
   ],
   "source": [
    "# Data quality check to ensure the data is correclty populated\n",
    "cur.execute(\"SELECT COUNT(1) FROM fact_fao\")\n",
    "print(\"nº of rows:\", cur.fetchone()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the data dictionary\n",
    "data_dictionary = {\n",
    "    \"table_countries\": {\n",
    "        \"table_type\": \"Dimension\",\n",
    "        \"column_Area_Code\": {\n",
    "            \"data type\": \"int\",\n",
    "            \"description\": \"numerical identifier of country\",\n",
    "            \"example\": \"132\"\n",
    "        },\n",
    "        \"column_Area\": {\n",
    "            \"data type\": \"varchar(100)\",\n",
    "            \"description\": \"name of the country\",\n",
    "            \"example\": \"Spain\"\n",
    "        }\n",
    "    },\n",
    "    \"table_items\": {\n",
    "        \"table_type\": \"Dimension\",\n",
    "        \"column_Item_Code\": {\n",
    "            \"data type\": \"int\",\n",
    "            \"description\": \"numerical identifier of item\",\n",
    "            \"example\": \"56\"\n",
    "        },\n",
    "        \"column_Item\": {\n",
    "            \"data type\": \"varchar(100)\",\n",
    "            \"description\": \"item name, for our use case, an item is a crop\",\n",
    "            \"example\": \"Wheat\"\n",
    "        }\n",
    "    },\n",
    "    \"table_elements\": {\n",
    "        \"table_type\": \"Dimension\",\n",
    "        \"column_Element_Code\": {\n",
    "            \"data type\": \"int\",\n",
    "            \"description\": \"numerical identifier of element\",\n",
    "            \"example\": \"5312\"\n",
    "        },\n",
    "        \"column_Element\": {\n",
    "            \"data type\": \"varchar(100)\",\n",
    "            \"description\": \"element name, in our use case, an element corresponds to the type of data\",\n",
    "            \"example\": \"Area harvested\"\n",
    "        }\n",
    "    },\n",
    "    \"table_flags\": {\n",
    "        \"table_type\": \"Dimension\",\n",
    "        \"column_Flag\": {\n",
    "            \"data type\": \"varchar(25)\",\n",
    "            \"description\": \"abreviation or representative string of flag as identifier\",\n",
    "            \"example\": \"A\"\n",
    "        },\n",
    "        \"column_Description\": {\n",
    "            \"data type\": \"varchar(100)\",\n",
    "            \"description\": \"full description of flag\",\n",
    "            \"example\": \"Aggregated data\"\n",
    "        }\n",
    "    },\n",
    "    \"table_fact_fao\": {\n",
    "        \"table_type\": \"Fact\",\n",
    "        \"column_Area_Code\": {\n",
    "            \"data type\": \"int\",\n",
    "            \"description\": \"numerical identifier of country\",\n",
    "            \"example\": \"132\"\n",
    "        },\"column_Item_Code\": {\n",
    "            \"data type\": \"int\",\n",
    "            \"description\": \"numerical identifier of item\",\n",
    "            \"example\": \"56\"\n",
    "        },\"column_Element_Code\": {\n",
    "            \"data type\": \"int\",\n",
    "            \"description\": \"numerical identifier of element\",\n",
    "            \"example\": \"5312\"\n",
    "        },\n",
    "        \"column_Unit\": {\n",
    "            \"data type\": \"varchar(50)\",\n",
    "            \"description\": \"measurement unit of value\",\n",
    "            \"example\": \"tonnes\"\n",
    "        },\n",
    "        \"column_Year\": {\n",
    "            \"data type\": \"int\",\n",
    "            \"description\": \"year of the data reported\",\n",
    "            \"example\": \"1961\"\n",
    "        },\n",
    "        \"column_Value\": {\n",
    "            \"data type\": \"float\",\n",
    "            \"description\": \"value to evaluate\",\n",
    "            \"example\": \"4523.43\"\n",
    "        },\n",
    "        \"column_Flag\": {\n",
    "            \"data type\": \"varchar(25)\",\n",
    "            \"description\": \"abreviation or representative string of flag as identifier\",\n",
    "            \"example\": \"A\"\n",
    "        },\n",
    "    }\n",
    "}"
   ]
  },
  {
   "source": [
    "# Done!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}